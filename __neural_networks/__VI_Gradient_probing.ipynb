{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VI Gradient_probing\n",
    "\n",
    "We went through how to do <code>**forward and back propogation**</code> in neural network in order to compute derivatives. But back pro as an algoritmh has realy a lot of details and is tricky to implement corecctly. Unfortunate propery is that there are many ways to have bugs in back prop. When gradien descent is applied or some other optimization algorithm it could actually look like it's working and cost function ùêΩ(Œò) may end up decreasing on every iteration of gradient descent. But this could prove true even though there might be some bug in your implementation of back propogation end we might end up with a neural network that has a higher level of error thant you would have without bugs. And we might just not know that there was some bugs that are giving us worse performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>**Gradient checking**</code><br>\n",
    "\n",
    "A method that alleviates this problem. Every time we implement back propagation or a similar gradient descent algorithm on a neural network or any other reasonably complex model it reasonable to implement gradient checking. It gives us a high confidence that our implementation of forward and back propagation is correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 32-bit",
   "language": "python",
   "name": "python37432bitc856d1d617f0478da8cbf97a005d9730"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
