{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II Forward propagation\n",
    "\n",
    "So we have some mathematical definition how to represent and how to compute the hypotheses used by neural network.\n",
    "Based on that concept, we are going to design some NN model and carry out that computation efficiently and see how can it be applied to non-linear classification problems.\n",
    "\n",
    "\n",
    "## Vectorized inplementation\n",
    "\n",
    "Previousy we said that the sequance of steps we need in order to compute the output of a hypotheses are these equations\n",
    "\n",
    "\\begin{multline*}\n",
    "a_{1}^{ (2) } = g( \\Theta_{10}^{(1)}x_{0} + \\Theta_{11}^{(1)}x_{1} + \\Theta_{12}^{(1)}x_{2} +  \\Theta_{13}^{(1)}x_{3} ) \n",
    "\\end{multline*}\n",
    "\n",
    "\\begin{multline*}\n",
    "a_{2}^{ (2) } = g( \\Theta_{20}^{(1)}x_{0} + \\Theta_{21}^{(1)}x_{1} + \\Theta_{22}^{(1)}x_{2} +  \\Theta_{23}^{(1)}x_{3} ) \n",
    "\\end{multline*}\n",
    "\n",
    "\\begin{multline*}\n",
    "a_{3}^{ (2) } = g( \\Theta_{30}^{(1)}x_{0} + \\Theta_{31}^{(1)}x_{1} + \\Theta_{32}^{(1)}x_{2} +  \\Theta_{33}^{(1)}x_{3} )\n",
    "\\end{multline*}\n",
    "\n",
    "\\begin{multline*}\n",
    "h_{\\Theta}(x) = a_{1}^{(3)} = g(\\Theta_{10}^{(2)} a_{0}^{(2)} + \\Theta_{11}^{(2)}a_{1}^{(2)} + \\Theta_{12}^{(2)}a_{2}^{(2)} + \\Theta_{13}^{(2)}a_{3}^{(2)})\n",
    "\\end{multline*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the acticvation values of the <code>**3 hidden units**</code> and the we use those to compute the final output of our hypotheses. We need to define few extra terms to help us compact things a bit\n",
    "\n",
    "\\begin{multline*}\n",
    "a_{1}^{ (2) } = g(\\Theta_{10}^{(1)}x_{0} + \\Theta_{11}^{(1)}x_{1} + \\Theta_{12}^{(1)}x_{2} +  \\Theta_{13}^{(1)}x_{3})  \\\n",
    "\\ ‚Üí \\ a_{1}^{ (2)} = g(z_{1}^{(2)})\n",
    "\\end{multline*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>**NOTE:That supper script value is asociated with the layer in the neural network**</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{multline*}\n",
    "a_{1}^{ (2)} = g(z_{1}^{(2)})\n",
    "\\end{multline*}\n",
    "\n",
    "\\begin{multline*}\n",
    "a_{2}^{ (2)} = g(z_{2}^{(2)})\n",
    "\\end{multline*}\n",
    "\n",
    "\\begin{multline*}\n",
    "a_{3}^{ (2)} = g(z_{3}^{(2)})\n",
    "\\end{multline*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our Œò is a block of numbers it can be a is matrix vector so <code>**matrix vector multiplication Œò‚ä§ùëã**</code>  is applicable. Using this observation we are going to be able to vectorize this computation of NN. <br>\n",
    "\n",
    "Lets define feature vector x and z2 to be the vector of those z values.\n",
    "\n",
    "\\begin{multline*}\n",
    "x = \n",
    "\\begin{bmatrix}\n",
    "x_{0} \\\\\n",
    "x_{1} \\\\\n",
    "x_{2} \\\\\n",
    "x_{3}\n",
    "\\end{bmatrix}\n",
    "\\\n",
    "z^{(2)} =\n",
    "\\begin{bmatrix}\n",
    "z_{1}^{(2)} \\\\\n",
    "z_{2}^{(2)} \\\\\n",
    "z_{3}^{(2)}\n",
    "\\end{bmatrix}\n",
    "\\end{multline*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can vectorize the computation of ùëé(2)1, ùëé(2)2, ùëé(2)3 as follows:\n",
    "\n",
    "\\begin{multline*}\n",
    "1)\\ z^{(2)} = \\Theta^{(1)}x\n",
    "\\end{multline*}\n",
    "\n",
    "\\begin{multline*}\n",
    "2)\\ a^{(2)} = g(z^{(2)})\n",
    "\\end{multline*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In first step it will gives us the ùëß(2) vector and then ùëé(2) is just the activation of  ùëß(2).  ùëß(2) is a 3dimensional vector and ùëé(2) as well\n",
    "\n",
    "\\begin{multline*}\n",
    "z^{(2)} \\in \\mathbb{R^{3}} , \\ \n",
    "a^{(2)} \\in \\mathbb{R^{3}}\n",
    "\\end{multline*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And thus this we can apply activation (sigmoid function) element wise to each of ùëß(2) elements. To make our notation a bit more consistent in the inpute layer we have inputs x but we cal also think about it as activation of the first layers. \n",
    "\n",
    "\\begin{multline*}\n",
    "a^{(1)} = x\n",
    "\\end{multline*}\n",
    "\n",
    "So ùëé(1) is a vector now we can replace it here just by defining a(1) to be activations in our input layer.\n",
    "\n",
    "\\begin{multline*}\n",
    "z^{(2)} = \\Theta^{(1)}a^{(1)}\n",
    "\\end{multline*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far  we got  the values \n",
    "\n",
    "\\begin{multline*}\n",
    "a^{(2)}_{1} \n",
    "\\end{multline*}\n",
    "\n",
    "\\begin{multline*}\n",
    "a^{(2)}_{2}\n",
    "\\end{multline*}\n",
    "\n",
    "\\begin{multline*}\n",
    "a^{(2)}_{3}\n",
    "\\end{multline*}\n",
    "\n",
    "But we still need one more values and that is:\n",
    "\n",
    "\\begin{multline*}\n",
    "a^{(2)}_{0}\n",
    "\\end{multline*}\n",
    "\n",
    "And that correspond to our <code>**\"bias unit\"**</code> in the hidden layer that goes to the output layer.\n",
    "\\begin{multline*}\n",
    "a^{(2)}_{0} = 1\n",
    "\\end{multline*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And after this step we now have that\n",
    "\n",
    "\\begin{multline*}\n",
    "a^{(2)} \\in \\mathbb{R^{4}}\n",
    "\\end{multline*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally to compute the actual value output of our hypothesis we then simply need to compute as follows:\n",
    "\n",
    "\\begin{multline*}\n",
    "1)\\ z^{(3)} = \\Theta^{(2)}a^{(2)}\n",
    "\\end{multline*}\n",
    "\n",
    "\\begin{multline*}\n",
    "2)\\ h_{\\Theta}(x) = a^{(3)} = g(z^{(3)})\n",
    "\\end{multline*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ùëß(3) is equal to this inner term:\n",
    "\n",
    "\\begin{multline*}\n",
    "\\Theta_{10}^{(2)} a_{0}^{(2)} + \\Theta_{11}^{(2)}a_{1}^{(2)} + \\Theta_{12}^{(2)}a_{2}^{(2)} + \\Theta_{13}^{(2)}a_{3}^{(2)}\n",
    "\\end{multline*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our final hypothesis output is ùëé(3) which is our sinlge output layer activation unit. So that just the real number.<br>\n",
    "This entire process is called <code>**forward propagation**</code>. Because we start of with the activations of the input units and then propagate forward to the hidden layer and compute the activations of that layer and then we propogate forward and compute the activations of our output layer and we worked out the <code>**vectorized implementation of this process**</code>\n",
    "\n",
    "This would give as an relatively efficient way of computing ‚ÑéŒò(ùë•)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural networks learning its own features\n",
    "\n",
    "TODO:\n",
    "___________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other network architectures\n",
    "\n",
    "TODO:\n",
    "_______________"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 32-bit",
   "language": "python",
   "name": "python37432bitc856d1d617f0478da8cbf97a005d9730"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
