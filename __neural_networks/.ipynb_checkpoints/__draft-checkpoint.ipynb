{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fminunc, conjugate gradient, BFGS, L-BFGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "with np.load(\"arrays.npz\") as data:\n",
    "\n",
    "    thrLayer = data['thrLayer'] \n",
    "\n",
    "    thetaO = data['thetaO']\n",
    "    thetaT = data['thetaT']\n",
    "\n",
    "    Ynew = data['Ynew']\n",
    "\n",
    "    X = data['X'] \n",
    "    Y = data['Y'] \n",
    "\n",
    "\n",
    "m = len(thrLayer)\n",
    "k = thrLayer.shape[1]\n",
    "\n",
    "\n",
    "#cost\n",
    "cost = 0\n",
    "Y_arr = np.zeros(Ynew.shape)\n",
    "for i in range(m):\n",
    "    Y_arr[ i, int(Y[i,0])-1] = 1\n",
    "\n",
    "for i in range(m):\n",
    "    for j in range(k):\n",
    "        cost += -Y_arr[i,j]*np.log(thrLayer[i,j]) - (1 - Y_arr[i,j])*np.log(1 - thrLayer[i,j])\n",
    "cost /= m\n",
    "\n",
    "\n",
    "#rCost \n",
    "rCost = 0\n",
    "for i in range(len(thetaO)):\n",
    "    for j in range(1,len(thetaO[0])):\n",
    "        rCost += thetaO[i,j]**2\n",
    "\n",
    "for i in range(len(thetaT)):\n",
    "    for j in range(1,len(thetaT[0])):\n",
    "        rCost += thetaT[i,j]**2\n",
    "lam=1\n",
    "rCost *= lam/(2.*m)\n",
    "\n",
    "\n",
    "print(cost)\n",
    "print(cost + rCost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r_cost(theta, X, y , lambda1):\n",
    "    current = X\n",
    "    for i in range(len(theta)):\n",
    "        a = np.append(np.ones((len(current),1)),current,axis=1)\n",
    "        z = np.dot(a,theta[i].T)\n",
    "        z = sigmoid(z)\n",
    "        current = z\n",
    "    htheta=current\n",
    "    \n",
    "    ans =np.sum(np.multiply(np.log(htheta),(y).T)) +  np.sum(np.multiply(np.log(1-htheta),(1-y).T))\n",
    "    ans= -ans/len(X)\n",
    "    \n",
    "    for i in range(len(theta)):\n",
    "        new=theta[i][:,1:]\n",
    "        newsum=np.sum(np.multiply(new,new))\n",
    "        ans+=newsum*(lambda1)/(2*len(X))\n",
    "\n",
    "    return ans\n",
    "\n",
    "print(r_cost(Θs,X,y,1))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NN:\n",
    "    def __init__(self, layers, alpha=0.1):\n",
    "        self.weights = []\n",
    "        self.layers  = layers\n",
    "        self.alpha   = alpha\n",
    "    \n",
    "        for i in np.arange(0, len(layers) - 2):\n",
    "            w = np.random.randn(layers[i] + 1, layers[i + 1] + 1)\n",
    "            self.weights.append(w / np.sqrt(layers[i]))\n",
    "        \n",
    "    def sigmoid(self, x):\n",
    "        return 1.0 / (1.0 + np.exp(-x))\n",
    "    \n",
    "    def sigmoid_deriv(self, x):\n",
    "        return x * (1 - x)\n",
    "    \n",
    "    def fit_partial(self, x, y):\n",
    "        A = [np.atleast_2d(x)]\n",
    "        \n",
    "        for layer in np.arange(0, len(self.weights )):\n",
    "            net = A[layer].dot(self.weights[layer])\n",
    "            out = self.sigmoid(net)\n",
    "            A.append(out)\n",
    "            error = A[-1] - y\n",
    "            D = [error * self.sigmoid_deriv(A[-1])]\n",
    "            \n",
    "            for layer in np.arange(len(A) - 2, 0, -1):\n",
    "                delta = D[-1].dot(self.weights[layer].T)\n",
    "                delta = delta * self.sigmoid_deriv(A[layer])\n",
    "                D.append(delta)\n",
    "            \n",
    "            D = D[::-1]\n",
    "            \n",
    "            for layer in np.arange(0, len(self.weights)):\n",
    "                self.weights[layer] += -self.alpha * A[layer].T.dot(D[layer])\n",
    "                \n",
    "    def fit(self, X, y, epochs=1000, displayUpdate=100):\n",
    "        m, n = X.shape\n",
    "        X = np.c_[X, np.ones((m))]\n",
    "        for epoch in np.arange(0, epochs):\n",
    "            for (x, label) in zip(X, y):\n",
    "                self.fit_partial(x, label)\n",
    "                if epoch == 0 or (epoch + 1) % displayUpdate == 0:\n",
    "                    loss = self.calculate_loss(X, y)\n",
    "                    print(f\"epoch={epoch + 1}, loss={loss}\")\n",
    "                \n",
    "                \n",
    "    def predict(self, X, addBias=True):\n",
    "        p = np.atleast_2d(X)\n",
    "        m, n = p.shape\n",
    "        \n",
    "        if addBias:\n",
    "           p = np.c_[p, np.ones((m))]\n",
    "            \n",
    "        for layer in np.arange(0, len(self.weights)):\n",
    "            p = self.sigmoid(np.dot(p, self.weights[layer]))\n",
    "            \n",
    "        return p\n",
    "    \n",
    "    def calculate_loss(self, X, labels):\n",
    "        labels = np.atleast_2d(labels)\n",
    "        predictions = self.predict(X, addBias=False)\n",
    "        return (0.5 * np.sum((predictions - labels) ** 2))\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [1], [1], [0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NN([2, 2, 1], alpha=0.5)\n",
    "nn.fit(X, y, epochs=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (x, target) in zip(X, y):\n",
    "    pred = nn.predict(x)[0][0]\n",
    "    step = 1 if pred > 0.5 else 0\n",
    "    print(f\"data={x}, true={target[0]}, pred={pred}, step={step}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NN:\n",
    "    def __init__(self, layers, alpha=0.1):\n",
    "        self.weights = []\n",
    "        self.layers  = layers\n",
    "        self.alpha   = alpha\n",
    "        self.weights = [np.random.randn(y, x) / np.sqrt(x) for x, y in zip(layers[:-1], layers[1:])]\n",
    "        self.biases  = [np.random.randn(y, 1) for y in layers[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "from scipy import optimize as opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('bmh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loadmat('data/ex4data1.mat')\n",
    "theta = loadmat('data/ex4weights.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['X']\n",
    "y = data['y']\n",
    "theta1 = theta[ 'Theta1' ]\n",
    "theta2 = theta[ 'Theta2' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unroll parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_n = pd.get_dummies(y.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_params = np.hstack((theta1.ravel(order='F'), theta2.ravel(order='F'))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_prime(z):\n",
    "    return np.multiply(sigmoid(z), 1-sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedforward(theta1, theta2, m, X):\n",
    "     bias = np.ones((m,1))\n",
    "     a1   = np.hstack((bias, X))\n",
    "     a2   = sigmoid(a1.dot(theta1.T))\n",
    "     a2   = np.hstack((bias, a2))\n",
    "     a3   = sigmoid(a2.dot(theta2.T))\n",
    "     h    = a3;\n",
    "     return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, λ=1):\n",
    "    \n",
    "    m = len(y)\n",
    "    _slice = hidden_layer_size* (input_layer_size + 1)\n",
    "  \n",
    "    th1 = np.reshape(\n",
    "        nn_params[: _slice], (hidden_layer_size, input_layer_size  + 1), order = 'F')\n",
    "\n",
    "    th2 = np.reshape(\n",
    "        nn_params[ _slice:], (num_labels,        hidden_layer_size + 1), order = 'F')\n",
    "    \n",
    "    h = feedforward(th1, th2, m, X)\n",
    "    \n",
    "    # cost \n",
    "    \n",
    "    ans= np.sum(np.multiply(np.log(htheta),(y).T)) + np.sum(np.multiply(np.log(1-htheta),(1-y).T))\n",
    "    \n",
    "    \n",
    "    cost = np.sum((np.multiply(y, np.log(h))) + (np.multiply(1-y, np.log(1-h))))\n",
    "    \n",
    "    # regularization\n",
    "    L2_on_TH1 = np.power(th1[:,1:],2)\n",
    "    L2_on_TH2 = np.power(th2[:,1:],2)\n",
    "    \n",
    "    reg_sum_1 = np.sum(np.sum(L2_on_TH1, axis = 1))\n",
    "    reg_sum_2 = np.sum(np.sum(L2_on_TH2, axis = 1))\n",
    "    \n",
    "    return np.sum(cost / (-m)) + (reg_sum_1 + reg_sum_2) * lmbda / (2*m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer_size = 400\n",
    "hidden_layer_size = 25\n",
    "num_labels = 10\n",
    "lmbda = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38376985909092354"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y_n, lmbda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randInitializeWeights(L_in, L_out):\n",
    "    i = 0.12\n",
    "    return np.random.rand(L_out, L_in + 1) * 2 * i - i\n",
    "\n",
    "initial_theta1 = randInitializeWeights(input_layer_size, hidden_layer_size)\n",
    "initial_theta2 = randInitializeWeights(hidden_layer_size, num_labels)\n",
    "\n",
    "# unrolling parameters into a single column vector\n",
    "nn_initial_params = np.hstack((initial_theta1.ravel(order='F'), initial_theta2.ravel(order='F')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10285"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropagation sequance:\n",
    "\n",
    "* 1) Compute the forward propagate to get the output activation a3;\n",
    "* 2) Calculate the error term 𝛿3 that’s obtained by subtracting actual output from our calculated output a3;\n",
    "* 3) For hidden layer, error term 𝛿2 can be calculated as below:\n",
    "\n",
    "\\begin{multline*}\n",
    "\\delta^{(2)} = (\\Theta^{(2)})^{\\top} \\delta^{(3)} \\odot  g’(z^{(2)})\n",
    "\\end{multline*}\n",
    "\n",
    "* 4) Accumulate the gradients in 𝛿1 and 𝛿2;\n",
    "* 5) Obtain the gradients for the neural network by diving the accumulated gradients (of step 4) by m;\n",
    "* 6) Add the regularization terms to the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nnGrad(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, lmbda):\n",
    "    \n",
    "    m, n   = X.shape\n",
    "    _slice = hidden_layer_size* (input_layer_size + 1)\n",
    "    \n",
    "    theta1 = np.reshape(nn_params[:_slice], (hidden_layer_size, input_layer_size + 1), order='F')\n",
    "    theta2 = np.reshape(nn_params[_slice:], (num_labels, hidden_layer_size + 1), order='F')\n",
    "    \n",
    "    delta1 = np.zeros(theta1.shape)\n",
    "    delta2 = np.zeros(theta2.shape)\n",
    "    \n",
    "  \n",
    "    # NOT VECTORIZED. \n",
    "    \n",
    "    for i in range(m):\n",
    "        ones = np.ones(1)\n",
    "        a1 = np.hstack((ones, X[i]))\n",
    "        z2 = a1.dot(theta1.T)\n",
    "        a2 = np.hstack((ones, sigmoid(z2)))\n",
    "        z3 = a2.dot(theta2.T)\n",
    "        a3 = sigmoid(z3)\n",
    "\n",
    "        d3 = a3 - y.iloc[i,:][np.newaxis,:]\n",
    "        \n",
    "        z2 = np.hstack((ones, z2))\n",
    "        \n",
    "        d2 = np.multiply((theta1.T).dot(d3.T), sigmoid_prime(z2).T[:,np.newaxis])\n",
    "        \n",
    "        delta1 = delta1 + d2[1:,:].dot(a1[np.newaxis,:])\n",
    "        delta2 = delta2 + d3.T.dot( a2[np.newaxis,:])\n",
    "        \n",
    "    delta1 /= m\n",
    "    delta2 /= m\n",
    "    #print(delta1.shape, delta2.shape)\n",
    "    delta1[:,1:] = delta1[:,1:] + theta1[:,1:] * lmbda / m\n",
    "    delta2[:,1:] = delta2[:,1:] + theta2[:,1:] * lmbda / m\n",
    "        \n",
    "    return np.hstack((delta1.ravel(order='F'), delta2.ravel(order='F')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nnGrad(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, lmbda):\n",
    "    \n",
    "    initial_theta1 = np.reshape(nn_params[:hidden_layer_size*(input_layer_size+1)], (hidden_layer_size, input_layer_size+1), 'F')\n",
    "    initial_theta2 = np.reshape(nn_params[hidden_layer_size*(input_layer_size+1):], (num_labels, hidden_layer_size+1), 'F')\n",
    "    delta1 = np.zeros(initial_theta1.shape)\n",
    "    delta2 = np.zeros(initial_theta2.shape)\n",
    "    m = len(y)\n",
    "    \n",
    "    for i in range(X.shape[0]):\n",
    "        ones = np.ones(1)\n",
    "        a1 = np.hstack((ones, X[i]))\n",
    "        z2 = a1 @ initial_theta1.T\n",
    "        a2 = np.hstack((ones, sigmoid(z2)))\n",
    "        z3 = a2 @ initial_theta2.T\n",
    "        a3 = sigmoid(z3)\n",
    "\n",
    "        d3 = a3 - y.iloc[i,:][np.newaxis,:]\n",
    "        z2 = np.hstack((ones, z2))\n",
    "        d2 = np.multiply(initial_theta2.T @ d3.T, sigmoid_prime(z2).T[:,np.newaxis])\n",
    "        delta1 = delta1 + d2[1:,:] @ a1[np.newaxis,:]\n",
    "        delta2 = delta2 + d3.T @ a2[np.newaxis,:]\n",
    "        \n",
    "    delta1 /= m\n",
    "    delta2 /= m\n",
    "    #print(delta1.shape, delta2.shape)\n",
    "    delta1[:,1:] = delta1[:,1:] + initial_theta1[:,1:] * lmbda / m\n",
    "    delta2[:,1:] = delta2[:,1:] + initial_theta2[:,1:] * lmbda / m\n",
    "        \n",
    "    return np.hstack((delta1.ravel(order='F'), delta2.ravel(order='F')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Eivanas\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:17: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n"
     ]
    }
   ],
   "source": [
    "nn_backprop_Params = nnGrad(nn_initial_params, input_layer_size, hidden_layer_size, num_labels, X, y_n, lmbda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.03501864, -0.00700024, -0.0350988 , ...,  0.1857535 ,\n",
       "        0.13644557,  0.15601988])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_backprop_Params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first half of the function is calculating the error by running the data plus current parameters through the\n",
    "\"network\" (the forward-propagate function) and comparing the output to the true labels. The total error across the\n",
    "whole data set is represented as J. The rest of the function is essentially answering the question \"how can I adjust my parameters to reduce the error the next time I run through the network\"? It does this by computing the contributions at each layer to the total error and adjusting appropriately by coming up with a \"gradient\" matrix (or, how much to change each parameter and in what direction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from scipy.io import loadmat\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "\n",
    "def load_data(filename):\n",
    "    try:\n",
    "        return loadmat(filename)\n",
    "    except TypeError:\n",
    "        print(\"invalid filename: \" + filename)\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp((-x)))\n",
    "\n",
    "\n",
    "def sigmoid_gradient(z):\n",
    "    sig_z = sigmoid(z)\n",
    "    return np.multiply(sig_z, (1 - sig_z))\n",
    "\n",
    "\n",
    "def forwardprop(X, theta1, theta2):\n",
    "    m, n = X.shape\n",
    "    bias = np.ones((m,1))\n",
    "    a1   = np.hstack((bias, X))\n",
    "    z2   = a1.dot(theta1.T)\n",
    "    a2   = np.hstack((bias, sigmoid(z2)))\n",
    "    z3   = a2.dot(theta2.T)\n",
    "    a3   = sigmoid(z3)\n",
    "    h    = a3\n",
    "    return a1, z2, a2, z3, h\n",
    "\n",
    "\n",
    "def unroll(scissors, params, ils, hls, nl):\n",
    "    theta1 = np.reshape(\n",
    "        params[:scissors], (hls, (ils + 1))\n",
    "    )\n",
    "    theta2 = np.reshape(\n",
    "        params[scissors:], (nl, (hls + 1))\n",
    "    )\n",
    "    \n",
    "    return theta1, theta2\n",
    "\n",
    "def backprop(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, learning_rate, regularize = True):\n",
    "\n",
    "    m, n  = X.shape\n",
    "    bias = np.ones((m,1))\n",
    "    _slice = hidden_layer_size * (input_layer_size + 1)\n",
    "    \n",
    "    theta1, theta2  = unroll(_slice, nn_params, input_layer_size, hidden_layer_size, num_labels)\n",
    "    a1, z2, a2, z3, h = forwardprop(X, theta1, theta2)\n",
    "\n",
    "    # initializations\n",
    "    delta1 = np.zeros(theta1.shape)  # (25, 401)\n",
    "    delta2 = np.zeros(theta2.shape)  # (10, 26)\n",
    "    J = cost(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, learning_rate, regularize)\n",
    "\n",
    "    d3 = h - y  # (5000, 10)\n",
    "    \n",
    "    z2 = np.hstack((z2, bias))\n",
    "\n",
    "    d2 = np.multiply((theta2.T @ d3.T).T, sigmoid_gradient(z2))  # (5000, 26)\n",
    "\n",
    "    delta1 += (d2[:, 1:]).T @ a1\n",
    "    delta2 += d3.T @ a2\n",
    "\n",
    "    delta1 = delta1 / m\n",
    "    delta2 = delta2 / m\n",
    "\n",
    "    # add regularization term if needed\n",
    "    if regularize:\n",
    "        delta1[:, 1:] = delta1[:, 1:] + (theta1[:, 1:] * learning_rate) / m\n",
    "        delta2[:, 1:] = delta2[:, 1:] + (theta2[:, 1:] * learning_rate) / m\n",
    "\n",
    "    # unravel the gradient matrices into a single array\n",
    "    grad = np.concatenate((np.ravel(delta1), np.ravel(delta2)))\n",
    "\n",
    "    return J, grad\n",
    "\n",
    "\n",
    "def cost(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, learning_rate, regularize=True):\n",
    "    m, n  = X.shape\n",
    "    _slice = hidden_layer_size * (input_layer_size + 1)\n",
    "    theta1, theta2 = unroll(_slice, nn_params, input_layer_size, hidden_layer_size, num_labels)\n",
    "    \n",
    "    h = forwardprop(X, theta1, theta2)[4]\n",
    "\n",
    "    J = (np.multiply(-y, np.log(h)) - np.multiply((1 - y), np.log(1 - h))).sum() / m\n",
    "\n",
    "    if regularize:\n",
    "        J += (float(learning_rate) /\n",
    "              (2 * m)) * (np.sum(np.power(theta1[:, 1:], 2)) + np.sum(np.power(theta2[:, 1:], 2)))\n",
    "\n",
    "    return J\n",
    "\n",
    "\n",
    "def init_params(_in, _out, epsilon=0.12):\n",
    "    return np.random.randn(_out, _in + 1) * 2 * epsilon - epsilon\n",
    "\n",
    "def run_net():\n",
    "\n",
    "    input_size = 400\n",
    "    hidden_size = 25\n",
    "    num_labels = 10\n",
    "    learning_rate = 1\n",
    "\n",
    "    data = load_data('data/ex4data1.mat')\n",
    "    X = data['X']  \n",
    "    y = data['y']  \n",
    "\n",
    "    print(X.shape, y.shape)\n",
    "\n",
    "    encoder = OneHotEncoder(sparse=False)\n",
    "    y_encoded = encoder.fit_transform(y)\n",
    "\n",
    "    print(y_encoded.shape)\n",
    "\n",
    "    \n",
    "    theta1 = init_params(input_layer_size,  hidden_layer_size)\n",
    "    theta2 = init_params(hidden_layer_size, num_labels       )\n",
    "    \n",
    "    params = np.hstack((theta1.ravel(order='F'), theta2.ravel(order='F')))\n",
    "\n",
    "    print(\"Running the minimization algorithm for the neural net backpropagation algorithm...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    fmin = minimize(fun=backprop, x0=params, args=(input_size, hidden_size, num_labels, X, y_encoded, learning_rate),\n",
    "                    method='TNC', jac=True, options={'maxiter': 250})\n",
    "\n",
    "    print(\"The minimization result: \", fmin)\n",
    "\n",
    "    X = np.matrix(X)\n",
    "    theta1 = np.matrix(np.reshape(fmin.x[:hidden_size * (input_size + 1)], (hidden_size, (input_size + 1))))\n",
    "    theta2 = np.matrix(np.reshape(fmin.x[hidden_size * (input_size + 1):], (num_labels, (hidden_size + 1))))\n",
    "\n",
    "    a1, z2, a2, z3, h = forwardprop(X, theta1, theta2)\n",
    "    y_pred = np.array(np.argmax(h, axis=1) + 1)\n",
    "\n",
    "    print(\"The network predicts: \", y_pred)\n",
    "\n",
    "    correct = [1 if a == b else 0 for (a, b) in zip(y_pred, y)]\n",
    "    accuracy = (sum(map(int, correct)) / float(len(correct)))\n",
    "\n",
    "    print('accuracy = {0}%'.format(accuracy * 100))\n",
    "    end_time = time.time() - start_time\n",
    "    print(\"optimization took: {0} seconds\".format(end_time))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 400) (5000, 1)\n",
      "(5000, 10)\n",
      "Running the minimization algorithm for the neural net backpropagation algorithm...\n",
      "The minimization result:       fun: 3.1089649256352714\n",
      "     jac: array([ 1.79492365e-02, -1.34956419e-05, -1.15251068e-04, ...,\n",
      "       -1.28476275e-04, -5.70148048e-05, -8.08697598e-02])\n",
      " message: 'Max. number of function evaluations reached'\n",
      "    nfev: 250\n",
      "     nit: 6\n",
      "  status: 3\n",
      " success: False\n",
      "       x: array([ 0.30894336, -0.06747821, -0.57625534, ..., -0.2676179 ,\n",
      "       -0.2171238 ,  0.4394968 ])\n",
      "The network predicts:  [[6]\n",
      " [5]\n",
      " [9]\n",
      " ...\n",
      " [4]\n",
      " [4]\n",
      " [8]]\n",
      "accuracy = 25.64%\n",
      "optimization took: 8.43283224105835 seconds\n"
     ]
    }
   ],
   "source": [
    "run_net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 32-bit",
   "language": "python",
   "name": "python37432bitc856d1d617f0478da8cbf97a005d9730"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
